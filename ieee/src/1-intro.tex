\section{Introduction}
\subsection{Motivation}\label{motivation}
Database management systems (DBMS) are an integral component of applications. One aspect of DBMS that makes it important in many applications is its adaptivity. DBMS has large amounts of configuration parameters that users can adjust in order to optimize to specific workloads and meet the user requirement. For instance, MySQL has about 190 parameters~\cite{mysql197}, while PostgreSQL offers about 170~\cite{postgre170, cdbtune}. Recent studies have put emphasis on building performance prediction models instead of relying on heuristics to find optimal configurations~\cite{iTune, Ottertune, Onlinetune, cdbtune, restune}.

While existing studies target to learn performance prediction models for unseen workloads, they assume that the model is learned from the same hardware environment as where it is used.
In reality, developers of a DBMS can test the performance of the DBMS with various configurations in a testing environment and build a performance prediction model to estimate the performance of the DBMS in the production environment\cite{Matsuura}.
Since a testing environment is often a scaled-down version of a production environment, the model learned from the testing environment may not be accurate in predicting the performance of the production environment.
What developers can do instead is to extract the knowledge of the testing environment that can be reused to build an accurate model for the production environment.

Such a model learning approach is an example of \textit{transfer learning}.
The goal of transfer learning is to reduce the learning cost (in above case, the sampling cost) in a target environment by utilizing the data from a different but related environment~\cite{l2s, datareuse}.
Studies have shown that different hardware environments exhibit similarities in the performance of DBMS, motivating the use of transfer learning techniques to exploit such similarities~\cite{Valov, jamshidi}.
% As for the developers' standpoint, by using the data from a testing environment, they can build a model with high accuracy using fewer samples from the target environment.

Our motivation is to apply transfer learning techniques to learn a DBMS performance prediction model using fewer samples from the target environment.

\subsection{Problem}
The application of transfer learning in the context of configurable software systems has been discussed\cite{l2s,Valov,jamshidi,datareuse}. 
% Each transfer learning approach takes advantage of source environment knowledge in different ways.
% For example, some studies claim that linear transformation of source model is sufficient to predict the performance in a target hardware environment as performance functions in two different environments have a linear relationship~\cite{Valov, jamshidi}.
% Another way to transfer knowledge is to include the data gathered from source environment with samples from target environment to learn the target model~\cite{datareuse}.
% There is also a systematic sampling strategy that selects the most informative samples from the target environment based on knowledge gained from source environment data.~\cite{l2s}.
While previous transfer learning approaches have been shown to be effective in certain configurable software systems, they do not consider the following DBMS parameters that must be taken into account in order to make the transfer of knowledge more effective.

\textbf{1. Parameters are not binary.}
Many of the previous studies focused their evaluation on cases in which the configuration parameters were binary, i.e., the parameter is either on or off~\cite{Valov, jamshidi, l2s}, but the parameters in DBMS often have a wide range of values.
For example, MySQL has a parameter that controls the amount of memory allocated to the buffer pool, and the value of the knob can range from 5 MB to the hardware limit~\cite{mysqlinnobuf}.
Under the assumption that source and target environments have different hardware limitations, this parameter is not even possible to be set to the same value that exceeds the limitations of the source environment.
Naively reusing the source data of such parameters would cause inaccuracies in the model, as the resulting model have no information about the performance of the parameter values that are specific to the target environment.
% Furthermore, since DBMS has numerous parameters with wide value ranges, merely sampling in a smart way is not enough to build a model effectively, as it would still require many samples to learn an accurate model for the vast configuration space.
\textbf{2. Parameters have different effects on performance in different environments.}
Although studies have shown that the performance functions of many parameters between two environments have a linear relationship~\cite{Valov, jamshidi}, employing a naive transfer learning strategy based on the assumption that all parameters follow this pattern can cause a significant bias in the model.
% Consider LineairDB\cite{lineairdb}, a database that has the number of threads as one of its parameters, for example.
Fig.~\ref{fig:valov} shows the relationship between the throughput of two environments for varying configurations in LineairDB.
From Fig.~\ref{fig:valov}, it is evident that the relationship is in fact not linear because two machines have different saturation points when the number of threads is varied.
% This example shows that some parameters are heavily machine dependent, having different effects on performance depending on the hardware limitations.

In order to build an accurate performance model of DBMS with fewer samples from the target environment, we need to take the above DBMS parameters into account by exploiting the knowledge of linear parameters while avoiding the inaccuracies caused by transferring the knowledge of non-linear parameters.
Nonetheless, to our knowledge, there is no existing transfer learning technique that considers such a design.

\subsection{Contribution}
We present ChimeraTL, a transfer learning pipeline that takes advantage of the knowledge of DBMS parameters to build an accurate performance model of DBMS with fewer samples from the target environment.
First, ChimeraTL starts by identifying two types of parameters: \textit{linear parameters} and \textit{non-linear parameters}.
Once the identification is done, ChimeraTL begins sampling the data from the target environment.
Initially, ChimeraTL focuses on collecting the data of linear parameters to learn a linear transformation between the source and target environments.
After there are enough data of linear parameters, ChimeraTL prioritizes sampling the data of non-linear parameters from the target environment.
With this strategy, ChimeraTL can reduce the number of samples while minimizing the bias caused by the difference between the source and target environments.

Central to ChimeraTL is its ability to identify which parameters are linear and which are non-linear.
Linear parameters are the ones that have similar performance effects across different environments, and non-linear parameters are the ones that have different performance effects depending on the hardware limitations as shown in Fig.~\ref{fig:valov}.
First, ChimeraTL prepares data of performance functions in two distinct environments (e.g. two docker containers with different resource restrictions in a testing environment), under the assumption that it is cheap to collect data outside the target environment. 
Then, it transforms the performance functions of a parameter to probability distributions and measure the similarity of the two distributions. 
% We use the Bhattacharyya distance~\cite{bhattacharyya} as a similarity measure in ChimeraTL as it is a symmetric measure that takes into account the difference of the whole distribution rather than the difference of mean or variance.

After the identification of the linear parameters, ChimeraTL exploits the knowledge of these parameters by applying a method that is a combination of linear transformation and data reuse.
It first samples data of the linear parameters from the target environment.
Using the sampled target data and the source data of the same parameters, ChimeraTL learns a linear transformation between the two environments.
ChimeraTL then linearly transforms the source data of linear parameters and uses them as additional training data to build a performance prediction model for the target environment.
By linearly transforming the source data, ChimeraTL can reuse the knowledge of the source environment while avoiding the bias caused by the difference between the source and target environments. 
During the above process, ChimeraTL samples the data of linear parameters more frequently than the other parameters.
However, the benefit of sampling the data of linear parameters diminishes after learning the linear transformation because the linear transformation can construct sufficient model of linear parameters in the target environment.
Once there are enough data of the linear parameters, ChimeraTL prioritizes sampling the data of non-linear parameters from the target environment.

Compared with learning the model from scratch in the target environment, ChimeraTL minimizes the prediction error to less than 10\% using 70\% fewer samples. None of the previously noted state-of-the-art transfer learning techniques\cite{l2s,Valov,datareuse} can achieve this level of accuracy with the same number of samples. 

% ChimeraTL's ability to build an accurate performance model of DBMS with fewer samples from the target environment can be applied in various scenarios.
% As mentioned in Section~\ref{motivation}, developers can use ChimeraTL to build a performance prediction model in a production environment using the data from a testing environment.
% Similarly, ChimeraTL can be used in a cloud environment where various hardware environments are available.
% Furthermore, ChimeraTL can be integrated into existing DBMS tuning systems that require pretrained models to build a performance prediction model in a new hardware environment\cite{Onlinetune,Ottertune}.
% In case the pretrained models are acquired from a different hardware environment, ChimeraTL can quickly adjust the pretrained models to the new hardware environment.
% Although ChimeraTL is designed primarily for DBMS, it can also be applied to other configurable software systems that have distinction between linear parameters and non-linear parameters.

% \subsection{Organization}
% The rest of this paper is organized as follows. 
% Section 2 revisits the transfer learning techniques used in previous studies. Section 3 describes the ChimeraTL pipeline in detail, followed by experimental evaluation in Section 4. Finally, we discuss the related work in Section 5 and conclude the paper and discuss future work in Section 6. 