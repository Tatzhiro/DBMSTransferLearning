\section{Related Work}
In the field of DBMS, performance prediction models are commonly used to recommend optimal configurations for dynamic workloads using past data\cite{Ottertune,Onlinetune,cdbtune,mysql197}. 
Despite the common use of performance models in DBMS, few studies, like ResTune, leverage knowledge from different hardware environments to build a performance prediction model\cite{restune}.
ResTune used a meta-learning approach to combine models from multiple environments.
However, the main concern of performance prediction models in DBMS is not on the accuracy itself but on finding optimal configurations for a new workload.

On the other hand, several research efforts in the field of transfer learning have aimed to efficiently learn models on new hardware environments. These studies have shown that software performance on one hardware environment has a strong correlation with that on another environment\cite{Valov,jamshidi}. It has also been found that parameters that significantly impact performance are likely to be shared between the source and target environments\cite{jamshidi,l2s}. However, these studies were based on the results of configuring binary parameters, and it was unclear whether the results of these studies were applicable to DBMS, where many parameters have a wide range of values.

In this paper, we found that some parameters in DBMS are linear, but others are not.
This difference in parameters limited the performance of existing transfer learning techniques that assumed a strong linear relationship between the source and target environments\cite{Valov,datareuse}.
Therefore, we proposed a new transfer learning technique that can build an accurate performance model of DBMS with fewer samples by considering the characteristics of DBMS parameters.